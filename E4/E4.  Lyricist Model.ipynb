{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "encouraging-greene",
   "metadata": {},
   "source": [
    "# E4. 멋진 작사가 만들기 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-treatment",
   "metadata": {},
   "source": [
    "### Step 0. 필요한 모듈 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surgical-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-answer",
   "metadata": {},
   "source": [
    "### Step 1. 데이터 다운로드\n",
    "\n",
    "이미 실습(1) 데이터 다듬기에서 Cloud shell에 심볼릭 링크로 ~/aiffel/lyricist/data를 생성하셨다면, ~/aiffel/lyricist/data/lyrics에 데이터가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-certificate",
   "metadata": {},
   "source": [
    "### Step 2. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eight-assumption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['At first I was afraid', 'I was petrified', 'I kept thinking I could never live without you', 'By my side But then I spent so many nights', \"Just thinking how you've done me wrong\", 'I grew strong', \"I learned how to get along And so you're back\", 'From outer space', 'I just walked in to find you', 'Here without that look upon your face I should have changed that fucking lock']\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-champagne",
   "metadata": {},
   "source": [
    "### Step 3. 데이터 정제\n",
    "\n",
    "* preprocess_sentence() 함수를 만든 것을 기억하시죠? 이를 활용해 데이터를 정제하도록 하겠습니다.\n",
    "* 추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거합니다. 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기 를 권합니다.\n",
    "* 단어장의 크기는 12,000 이상 으로 설정하세요! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-oxford",
   "metadata": {},
   "source": [
    "#### 3.1. 정규표현식을(Regex) 이용한 필터링 및 corpus 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "exempt-player",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2 특수문자 양쪽에 공백을 넣고\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4 a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "    sentence = sentence.strip() # 5 다시 양쪽 공백을 지웁니다\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "warming-bathroom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175986\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "static-throat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start> at first i was afraid <end>', '<start> i was petrified <end>', '<start> i kept thinking i could never live without you <end>', '<start> by my side but then i spent so many nights <end>', '<start> just thinking how you ve done me wrong <end>', '<start> i grew strong <end>', '<start> i learned how to get along and so you re back <end>', '<start> from outer space <end>', '<start> i just walked in to find you <end>', '<start> here without that look upon your face i should have changed that fucking lock <end>']\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-costume",
   "metadata": {},
   "source": [
    "#### 3.2. Corpus 를 Tokenize 시켜서 Tensor로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "headed-france",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   71  241 ...    0    0    0]\n",
      " [   2    5   57 ...    0    0    0]\n",
      " [   2    5 1087 ...    0    0    0]\n",
      " ...\n",
      " [   2   48   16 ...    0    0    0]\n",
      " [  25    9 2859 ...  264   19    3]\n",
      " [   2    6  181 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f1255d3add0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=30000, \n",
    "    filters=' ',\n",
    "    oov_token=\"<unk>\"\n",
    "    )\n",
    "    \n",
    "    # tokenizer 내부의 단어장 생성\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # corpus to Tensor\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen=15, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "intermediate-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted rows of which tensors contains 0 (padding).\n",
    "updated_tensor = tensor[np.any(tensor==0, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "reliable-omaha",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150198, 15)\n"
     ]
    }
   ],
   "source": [
    "print(updated_tensor.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-accountability",
   "metadata": {},
   "source": [
    "#### 3.3. Input 수정해주기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "unknown-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다. [row, column]\n",
    "# [:,:-1] all rows and last column\n",
    "src_input = updated_tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = updated_tensor[:, 1:]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "prescription-budget",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  71 241   5  57 665   3   0   0   0   0   0   0   0]\n",
      "[ 71 241   5  57 665   3   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(src_input[0])\n",
    "print(tgt_input[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "thirty-problem",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((1000, 14), (1000, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# increased batch size so that steps per epoch is not too big. \n",
    "BUFFER_SIZE = len(src_input) #142274\n",
    "BATCH_SIZE = 1000 \n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE # 142\n",
    "\n",
    "# 0:<pad>를 포함하여 +1\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-forwarding",
   "metadata": {},
   "source": [
    "### Step 4. 평가 데이터셋 분리\n",
    "\n",
    "총 데이터의 20% 를 평가 데이터셋으로 사용해 주세요!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "recognized-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size = 0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acute-program",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (120158, 14)\n",
      "Target Train: (120158, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-element",
   "metadata": {},
   "source": [
    "### Step 5. 인공지능 만들기\n",
    "* 모델의 Embedding Size와 Hidden Size를 조절\n",
    "* 10 Epoch 내외\n",
    "* val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계하세요! (Loss는 아래 제시된 Loss 함수를 그대로 사용!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-plant",
   "metadata": {},
   "source": [
    "#### Model 1\n",
    "same embedding size and hiddensize as LMS node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adjacent-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-switch",
   "metadata": {},
   "source": [
    "#### Model 2\n",
    "increased embedding size to 512. hidden size same as Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dental-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator2(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size2 = 512\n",
    "hidden_size2 = 1024\n",
    "model2 = TextGenerator2(tokenizer.num_words + 1, embedding_size2 , hidden_size2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-indonesian",
   "metadata": {},
   "source": [
    "#### Model 3\n",
    "increased embedding size to 1000 and increased hidden size to 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "incident-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator3(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size3 = 1000\n",
    "hidden_size3 = 1500\n",
    "model3 = TextGenerator3(tokenizer.num_words + 1, embedding_size3 , hidden_size3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-edinburgh",
   "metadata": {},
   "source": [
    "#### Model 1 fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "recent-idaho",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "150/150 [==============================] - 188s 1s/step - loss: 4.8133\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 185s 1s/step - loss: 3.2698\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 187s 1s/step - loss: 3.0669\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 188s 1s/step - loss: 2.9457\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 189s 1s/step - loss: 2.8619\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 189s 1s/step - loss: 2.7949\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 190s 1s/step - loss: 2.7279\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 189s 1s/step - loss: 2.6624\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 189s 1s/step - loss: 2.6109\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 189s 1s/step - loss: 2.5577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1254d3bf90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-cleanup",
   "metadata": {},
   "source": [
    "#### Model 2 fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "automotive-doubt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "150/150 [==============================] - 230s 2s/step - loss: 4.9126\n",
      "Epoch 2/8\n",
      "150/150 [==============================] - 229s 2s/step - loss: 3.3566\n",
      "Epoch 3/8\n",
      "150/150 [==============================] - 231s 2s/step - loss: 3.1455\n",
      "Epoch 4/8\n",
      "150/150 [==============================] - 231s 2s/step - loss: 3.0010\n",
      "Epoch 5/8\n",
      "150/150 [==============================] - 231s 2s/step - loss: 2.9027\n",
      "Epoch 6/8\n",
      "150/150 [==============================] - 231s 2s/step - loss: 2.8305\n",
      "Epoch 7/8\n",
      "150/150 [==============================] - 231s 2s/step - loss: 2.7632\n",
      "Epoch 8/8\n",
      "150/150 [==============================] - 231s 2s/step - loss: 2.6993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f12562c78d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model2.compile(loss=loss, optimizer=optimizer)\n",
    "model2.fit(dataset, epochs=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-improvement",
   "metadata": {},
   "source": [
    "#### Model 3 fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "banned-cross",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "150/150 [==============================] - 511s 3s/step - loss: 4.7749\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 511s 3s/step - loss: 3.3155\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 512s 3s/step - loss: 3.0978\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 512s 3s/step - loss: 2.9490\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 512s 3s/step - loss: 2.8397\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 512s 3s/step - loss: 2.7403\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 512s 3s/step - loss: 2.6592\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 512s 3s/step - loss: 2.5842\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 512s 3s/step - loss: 2.5102\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 512s 3s/step - loss: 2.4379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f11f001d690>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model3.compile(loss=loss, optimizer=optimizer)\n",
    "model3.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-evanescence",
   "metadata": {},
   "source": [
    "### Step 6. Model Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "arbitrary-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-communication",
   "metadata": {},
   "source": [
    "#### Model 1 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "united-building",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i m a <unk> <end> '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-person",
   "metadata": {},
   "source": [
    "#### Model 2 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "graphic-hampshire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model2, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-structure",
   "metadata": {},
   "source": [
    "#### Model 3 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "raised-holiday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model3, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-liabilities",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
